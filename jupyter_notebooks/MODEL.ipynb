{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Graph_regularization.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "24gYiJcWNlpA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Graph regularization for legal decisions classification using synthesized graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x6FJ64qMNLez"
      },
      "source": [
        "## Dependencies and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICGdZSjdfr2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --quiet neural-structured-learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import neural_structured_learning as nsl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AzBWdWkBqlMy"
      },
      "source": [
        "## Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kLSbRFguBUNl"
      },
      "source": [
        "### Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zsA8HuvvwGri",
        "colab": {}
      },
      "source": [
        "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
        "NBR_WEIGHT_SUFFIX = '_weight'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s8gMVBw6t6CI"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YlTmug7auQ2r",
        "colab": {}
      },
      "source": [
        "class HParams(object):\n",
        "    \"\"\"Hyperparameters used for training.\"\"\"\n",
        "    def __init__(self):\n",
        "        ### dataset parameters\n",
        "        self.num_classes = 253\n",
        "        self.max_seq_length = 2500\n",
        "        self.vocab_size = 87148\n",
        "        ### neural graph learning parameters\n",
        "        self.distance_type = nsl.configs.DistanceType.L2\n",
        "        self.graph_regularization_multiplier = 0.1\n",
        "        self.num_neighbors = 2\n",
        "        ### model architecture\n",
        "        self.num_embedding_dims = 16\n",
        "        self.num_lstm_dims = 64\n",
        "        self.num_fc_units = 64\n",
        "        ### training parameters\n",
        "        self.train_epochs = 5\n",
        "        self.batch_size = 128\n",
        "        ### eval parameters\n",
        "        self.eval_steps = None  # All instances in the test set are evaluated.\n",
        "\n",
        "HPARAMS = HParams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lFP_XKVRp4_S"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5lkZVynuHWs",
        "colab": {}
      },
      "source": [
        "def make_dataset(file_path, training=False):\n",
        "    \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
        "    Args:\n",
        "    file_path: Name of the file in the `.tfrecord` format containing\n",
        "      `tf.train.Example` objects.\n",
        "    training: Boolean indicating if we are in training mode.\n",
        "\n",
        "    Returns:\n",
        "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
        "    objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def pad_sequence(sequence, max_seq_length):\n",
        "        \"\"\"Pads the input sequence (a `tf.SparseTensor`) to `max_seq_length`.\"\"\"\n",
        "        pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n",
        "        padded = tf.concat([sequence.values, tf.fill((pad_size), tf.cast(0, sequence.dtype))], axis=0)\n",
        "        # The input sequence may be larger than max_seq_length. Truncate down if\n",
        "        # necessary.\n",
        "        return tf.slice(padded, [0], [max_seq_length])\n",
        "\n",
        "    def parse_example(example_proto):\n",
        "        \"\"\"Extracts relevant fields from the `example_proto`.\n",
        "\n",
        "        Args:\n",
        "          example_proto: An instance of `tf.train.Example`.\n",
        "\n",
        "        Returns:\n",
        "          A pair whose first value is a dictionary containing relevant features\n",
        "          and whose second value contains the ground truth labels.\n",
        "        \"\"\"\n",
        "        # The 'words' feature is a variable length word ID vector.\n",
        "        feature_spec = {'words': tf.io.VarLenFeature(tf.int64),\n",
        "                        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1)}\n",
        "        \n",
        "        # We also extract corresponding neighbor features in a similar manner to\n",
        "        # the features above during training.\n",
        "        if training:\n",
        "            for i in range(HPARAMS.num_neighbors):\n",
        "                nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "                nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n",
        "                                                 NBR_WEIGHT_SUFFIX)\n",
        "                feature_spec[nbr_feature_key] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "            # We assign a default value of 0.0 for the neighbor weight so that\n",
        "            # graph regularization is done on samples based on their exact number\n",
        "            # of neighbors. In other words, non-existent neighbors are discounted.\n",
        "                feature_spec[nbr_weight_key] = tf.io.FixedLenFeature([1], tf.float32, default_value=tf.constant([0.0]))\n",
        "\n",
        "        features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "        # Since the 'words' feature is a variable length word vector, we pad it to a\n",
        "        # constant maximum length based on HPARAMS.max_seq_length\n",
        "        features['words'] = pad_sequence(features['words'], HPARAMS.max_seq_length)\n",
        "        if training:\n",
        "            for i in range(HPARAMS.num_neighbors):\n",
        "                nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
        "                features[nbr_feature_key] = pad_sequence(features[nbr_feature_key], HPARAMS.max_seq_length)\n",
        "\n",
        "        labels = features.pop('label')\n",
        "        return features, labels\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset([file_path])\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(87148)\n",
        "    dataset = dataset.map(parse_example)\n",
        "    dataset = dataset.batch(HPARAMS.batch_size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg7aOT70gADA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV0lveWafIJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = make_dataset('nsl_train_data.tfr', True)\n",
        "test_dataset = make_dataset('test_data.tfr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-K9mQ4DfIJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpKOoWgu-llD",
        "colab": {}
      },
      "source": [
        "def make_bilstm_model():\n",
        "    \"\"\"Builds a bi-directional LSTM model.\"\"\"\n",
        "    inputs = tf.keras.Input(shape=(HPARAMS.max_seq_length,), dtype='int64', name='words')\n",
        "    embedding_layer = tf.keras.layers.Embedding(HPARAMS.vocab_size, HPARAMS.num_embedding_dims)(inputs)\n",
        "    lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(HPARAMS.num_lstm_dims))(embedding_layer)\n",
        "    dense_layer = tf.keras.layers.Dense(HPARAMS.num_fc_units, activation='relu')(lstm_layer)\n",
        "    outputs = tf.keras.layers.Dense(253, activation='softmax')(dense_layer)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WOEElnbtPzSr",
        "colab": {}
      },
      "source": [
        "# Build a new base LSTM model.\n",
        "base_reg_model = make_bilstm_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XGaDeyjEOMLC",
        "colab": {}
      },
      "source": [
        "# Wrap the base model with graph regularization.\n",
        "graph_reg_config = nsl.configs.make_graph_reg_config(\n",
        "    max_neighbors=HPARAMS.num_neighbors,\n",
        "    multiplier=HPARAMS.graph_regularization_multiplier,\n",
        "    distance_type=HPARAMS.distance_type,\n",
        "    sum_over_axis=-1)\n",
        "graph_reg_model = nsl.keras.GraphRegularization(base_reg_model,\n",
        "                                                graph_reg_config)\n",
        "graph_reg_model.compile(\n",
        "    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gSZSqJOKFdgX"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aONZhwc9FWoo",
        "colab": {}
      },
      "source": [
        "graph_reg_history = graph_reg_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=HPARAMS.train_epochs,\n",
        "    validation_data=test_dataset,\n",
        "    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xD1oHiGHFjPB"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vdFMEfe2e5JY",
        "colab": {}
      },
      "source": [
        "graph_reg_results = graph_reg_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)\n",
        "print(graph_reg_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3BshURAbF49R"
      },
      "source": [
        "### Create a graph of accuracy/loss over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kHxshrYLah9v",
        "colab": {}
      },
      "source": [
        "graph_reg_history_dict = graph_reg_history.history\n",
        "graph_reg_history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhjhH4n_aprb",
        "colab": {}
      },
      "source": [
        "acc = graph_reg_history_dict['accuracy']\n",
        "val_acc = graph_reg_history_dict['val_accuracy']\n",
        "loss = graph_reg_history_dict['loss']\n",
        "graph_loss = graph_reg_history_dict['graph_loss']\n",
        "val_loss = graph_reg_history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "# \"-r^\" is for solid red line with triangle markers.\n",
        "plt.plot(epochs, loss, '-r^', label='Training loss')\n",
        "# \"-gD\" is for solid green line with diamond markers.\n",
        "plt.plot(epochs, graph_loss, '-gD', label='Training graph loss')\n",
        "# \"-b0\" is for solid blue line with circle markers.\n",
        "plt.plot(epochs, val_loss, '-bo', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NE0vcDiqa1Id",
        "colab": {}
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, '-r^', label='Training acc')\n",
        "plt.plot(epochs, val_acc, '-bo', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}