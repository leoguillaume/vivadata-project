{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence, TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import neural_structured_learning as nsl\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from word_index import word_index\n",
    "from word_index_reverse import word_index_reverse\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_pickle('../data/preprocessing.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./doc_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = [TaggedDocument(row.tokens_rest, [row.ID]) for index, row in tqdm(df.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.build_vocab(tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(tagged_docs, total_examples=model.corpus_count, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "fname = './CAPP_model'\n",
    "model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = {ID: model.docvecs[ID] for ID in df.ID}\n",
    "df['doc_vectors'] = df.ID.map(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.main_labels.notnull()]\n",
    "labels_value_counts = data.main_labels.value_counts()\n",
    "quantiles = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "for i in range(len(quantiles) - 1):\n",
    "    \n",
    "    quantile_down = data.main_labels.value_counts().quantile(quantiles[i])\n",
    "    quantile_up = data.main_labels.value_counts().quantile(quantiles[i + 1])\n",
    "    D = data[data.main_labels.isin(labels_value_counts[labels_value_counts > quantile_down][labels_value_counts <= quantile_up].index)]\n",
    "    \n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f'Number of decisions per labels between {int(quantile_down)} and {int(quantile_up)}', fontweight = 'bold')\n",
    "\n",
    "    \n",
    "    M_mean = list()\n",
    "    M_median = list()\n",
    "    for label in D.main_labels.unique():\n",
    "        M = cosine_similarity(D.doc_vectors[D.main_labels == label].apply(pd.Series))\n",
    "        M_values = M[np.tril_indices_from(M, 1)]\n",
    "        M_mean.append(np.mean(M_values))\n",
    "        \n",
    "    sns.distplot(M_mean, hist = False)\n",
    "        \n",
    "    plt.xlabel(f'Mean: {np.mean(M_mean), 2}')\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.suptitle('Distribution of cosine similarity', fontsize=16, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../train_test_sets/'\n",
    "X_train = pd.read_pickle(filepath + 'X_train.pkl').reset_index(drop = True)\n",
    "X_test = pd.read_pickle(filepath + 'X_test.pkl').reset_index(drop = True)\n",
    "y_train = pickle.load(open(filepath + \"y_train.pkl\", \"rb\" ))\n",
    "y_test = pickle.load(open(filepath + \"y_test.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_words=87148`\n",
    "\n",
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Returns int64 tf.train.Feature.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value.tolist()))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns bytes tf.train.Feature.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns float tf.train.Feature.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_example(word_vector, record_id):\n",
    "    \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
    "    \n",
    "    features = {'id': _bytes_feature(str(record_id)),\n",
    "                'embedding': _float_feature(word_vector)}\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "def create_embeddings(model, IDs, output_path):\n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        for ID in IDs:\n",
    "            example = create_embedding_example(model.docvecs[ID], ID)\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Doc2Vec.load('../model/CAPP_model')\n",
    "create_embeddings(model = model, IDs = X_train.ID, output_path = '../outputs/train_embeddings.tfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nsl.tools.build_graph(['../outputs/train_embeddings.tfr'],\n",
    "                      '../outputs/train_graph_60.tsv',\n",
    "                      similarity_threshold=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nx_graph(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter = '\\t', header = None)\n",
    "    df = df.iloc[::2, :2]\n",
    "    dic_ = {}\n",
    "    for i in tqdm(df[0].unique()):\n",
    "        dic_[i] = df[df[0] == i][1].values.tolist()\n",
    "    G = nx.Graph(dic_)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = build_nx_graph('../outputs/train_graph_60.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of nodes: {G.number_of_nodes()}\\nNumber of edges: {G.number_of_edges()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos=pos, node_size = 6)\n",
    "plt.title('Synthetized graph', loc = 'left', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(word_vector, label, record_id):\n",
    "    \"\"\"Create tf.Example containing the sample's word vector, label, and ID.\"\"\"\n",
    "    features = {'id': _bytes_feature(str(record_id)),\n",
    "                'words': _int64_feature(np.asarray(word_vector)),\n",
    "                'label': _int64_feature(np.asarray([label]))}\n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "def create_records(data, labels, record_path):\n",
    "    with tf.io.TFRecordWriter(record_path) as writer:\n",
    "        for word_vector, record_id, label in tqdm(zip(data.token_vectors, data.ID, labels)):\n",
    "            example = create_example(word_vector, label, record_id)\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_records(X_train, y_train, '../outputs/train_data.tfr')\n",
    "create_records(X_test, y_test, '../outputs/test_data.tfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsl.tools.pack_nbrs('../outputs/train_data.tfr',\n",
    "                    '',\n",
    "                    '../outputs/train_graph_60.tsv',\n",
    "                    '../outputs/nsl_train_data.tfr',\n",
    "                    add_undirected_edges=True,\n",
    "                    max_nbrs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
